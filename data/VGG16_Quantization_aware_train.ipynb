{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f09907b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "     \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"VGG16_quant\"\n",
    "model = VGG16_quant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9e82df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG_quant(\n",
      "  (features): Sequential(\n",
      "    (0): QuantConv2d(\n",
      "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): QuantConv2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): QuantConv2d(\n",
      "      64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): QuantConv2d(\n",
      "      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): QuantConv2d(\n",
      "      128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantConv2d(\n",
      "      256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (25): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): QuantConv2d(\n",
      "      8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (28): ReLU(inplace=True)\n",
      "    (29): QuantConv2d(\n",
      "      8, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (30): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (33): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (38): ReLU(inplace=True)\n",
      "    (39): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (41): ReLU(inplace=True)\n",
      "    (42): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (43): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 200 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [70, 90]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "junior-reminder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 2.106 (2.106)\tData 0.385 (0.385)\tLoss 2.6414 (2.6414)\tPrec 12.500% (12.500%)\n",
      "Epoch: [0][200/391]\tTime 0.043 (0.051)\tData 0.003 (0.005)\tLoss 2.1741 (2.9873)\tPrec 13.281% (12.139%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.362 (0.362)\tLoss 2.1088 (2.1088)\tPrec 24.219% (24.219%)\n",
      " * Prec 18.910% \n",
      "best acc: 18.910000\n",
      "Epoch: [1][0/391]\tTime 0.497 (0.497)\tData 0.446 (0.446)\tLoss 2.1320 (2.1320)\tPrec 15.625% (15.625%)\n",
      "Epoch: [1][200/391]\tTime 0.041 (0.043)\tData 0.003 (0.005)\tLoss 1.9928 (2.0328)\tPrec 25.000% (20.095%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.412 (0.412)\tLoss 1.9649 (1.9649)\tPrec 25.000% (25.000%)\n",
      " * Prec 23.680% \n",
      "best acc: 23.680000\n",
      "Epoch: [2][0/391]\tTime 0.635 (0.635)\tData 0.587 (0.587)\tLoss 1.9722 (1.9722)\tPrec 25.781% (25.781%)\n",
      "Epoch: [2][200/391]\tTime 0.040 (0.043)\tData 0.003 (0.006)\tLoss 1.9223 (1.9170)\tPrec 25.000% (24.378%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.395 (0.395)\tLoss 1.8824 (1.8824)\tPrec 25.000% (25.000%)\n",
      " * Prec 30.480% \n",
      "best acc: 30.480000\n",
      "Epoch: [3][0/391]\tTime 0.575 (0.575)\tData 0.524 (0.524)\tLoss 1.7755 (1.7755)\tPrec 28.906% (28.906%)\n",
      "Epoch: [3][200/391]\tTime 0.051 (0.044)\tData 0.004 (0.006)\tLoss 1.8231 (1.8072)\tPrec 30.469% (29.252%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.408 (0.408)\tLoss 1.7517 (1.7517)\tPrec 35.938% (35.938%)\n",
      " * Prec 31.860% \n",
      "best acc: 31.860000\n",
      "Epoch: [4][0/391]\tTime 0.471 (0.471)\tData 0.420 (0.420)\tLoss 1.6968 (1.6968)\tPrec 42.969% (42.969%)\n",
      "Epoch: [4][200/391]\tTime 0.034 (0.042)\tData 0.003 (0.005)\tLoss 1.5905 (1.7065)\tPrec 39.062% (33.427%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.351 (0.351)\tLoss 1.7055 (1.7055)\tPrec 32.812% (32.812%)\n",
      " * Prec 32.870% \n",
      "best acc: 32.870000\n",
      "Epoch: [5][0/391]\tTime 0.530 (0.530)\tData 0.476 (0.476)\tLoss 1.7698 (1.7698)\tPrec 28.906% (28.906%)\n",
      "Epoch: [5][200/391]\tTime 0.035 (0.043)\tData 0.003 (0.005)\tLoss 1.6504 (1.6376)\tPrec 33.594% (36.517%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.416 (0.416)\tLoss 1.5264 (1.5264)\tPrec 45.312% (45.312%)\n",
      " * Prec 38.980% \n",
      "best acc: 38.980000\n",
      "Epoch: [6][0/391]\tTime 0.510 (0.510)\tData 0.460 (0.460)\tLoss 1.6288 (1.6288)\tPrec 33.594% (33.594%)\n",
      "Epoch: [6][200/391]\tTime 0.042 (0.043)\tData 0.003 (0.005)\tLoss 1.6809 (1.5711)\tPrec 31.250% (38.822%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.402 (0.402)\tLoss 1.5217 (1.5217)\tPrec 33.594% (33.594%)\n",
      " * Prec 40.790% \n",
      "best acc: 40.790000\n",
      "Epoch: [7][0/391]\tTime 0.588 (0.588)\tData 0.536 (0.536)\tLoss 1.4477 (1.4477)\tPrec 44.531% (44.531%)\n",
      "Epoch: [7][200/391]\tTime 0.034 (0.043)\tData 0.003 (0.005)\tLoss 1.4069 (1.4921)\tPrec 50.781% (42.541%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.413 (0.413)\tLoss 1.3686 (1.3686)\tPrec 43.750% (43.750%)\n",
      " * Prec 43.390% \n",
      "best acc: 43.390000\n",
      "Epoch: [8][0/391]\tTime 0.610 (0.610)\tData 0.562 (0.562)\tLoss 1.2935 (1.2935)\tPrec 50.000% (50.000%)\n",
      "Epoch: [8][200/391]\tTime 0.043 (0.044)\tData 0.003 (0.006)\tLoss 1.3602 (1.3722)\tPrec 47.656% (48.954%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.444 (0.444)\tLoss 1.3285 (1.3285)\tPrec 53.906% (53.906%)\n",
      " * Prec 52.240% \n",
      "best acc: 52.240000\n",
      "Epoch: [9][0/391]\tTime 0.558 (0.558)\tData 0.506 (0.506)\tLoss 1.2518 (1.2518)\tPrec 55.469% (55.469%)\n",
      "Epoch: [9][200/391]\tTime 0.035 (0.044)\tData 0.002 (0.006)\tLoss 1.3799 (1.2849)\tPrec 50.000% (52.977%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.392 (0.392)\tLoss 1.3296 (1.3296)\tPrec 53.125% (53.125%)\n",
      " * Prec 48.150% \n",
      "best acc: 52.240000\n",
      "Epoch: [10][0/391]\tTime 0.509 (0.509)\tData 0.458 (0.458)\tLoss 1.1490 (1.1490)\tPrec 59.375% (59.375%)\n",
      "Epoch: [10][200/391]\tTime 0.050 (0.042)\tData 0.004 (0.005)\tLoss 1.3615 (1.2027)\tPrec 56.250% (56.798%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.316 (0.316)\tLoss 1.2307 (1.2307)\tPrec 53.125% (53.125%)\n",
      " * Prec 53.860% \n",
      "best acc: 53.860000\n",
      "Epoch: [11][0/391]\tTime 0.613 (0.613)\tData 0.564 (0.564)\tLoss 1.0804 (1.0804)\tPrec 56.250% (56.250%)\n",
      "Epoch: [11][200/391]\tTime 0.040 (0.042)\tData 0.003 (0.006)\tLoss 1.0039 (1.1017)\tPrec 64.062% (60.005%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.304 (0.304)\tLoss 1.0477 (1.0477)\tPrec 59.375% (59.375%)\n",
      " * Prec 57.640% \n",
      "best acc: 57.640000\n",
      "Epoch: [12][0/391]\tTime 0.581 (0.581)\tData 0.535 (0.535)\tLoss 0.9086 (0.9086)\tPrec 73.438% (73.438%)\n",
      "Epoch: [12][200/391]\tTime 0.042 (0.042)\tData 0.003 (0.005)\tLoss 1.2813 (1.0346)\tPrec 56.250% (62.768%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.428 (0.428)\tLoss 0.8758 (0.8758)\tPrec 66.406% (66.406%)\n",
      " * Prec 62.550% \n",
      "best acc: 62.550000\n",
      "Epoch: [13][0/391]\tTime 0.505 (0.505)\tData 0.453 (0.453)\tLoss 1.0471 (1.0471)\tPrec 67.188% (67.188%)\n",
      "Epoch: [13][200/391]\tTime 0.038 (0.041)\tData 0.002 (0.005)\tLoss 0.8627 (0.9522)\tPrec 71.875% (66.231%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.291 (0.291)\tLoss 0.8363 (0.8363)\tPrec 70.312% (70.312%)\n",
      " * Prec 65.740% \n",
      "best acc: 65.740000\n",
      "Epoch: [14][0/391]\tTime 0.558 (0.558)\tData 0.507 (0.507)\tLoss 0.7438 (0.7438)\tPrec 70.312% (70.312%)\n",
      "Epoch: [14][200/391]\tTime 0.039 (0.043)\tData 0.003 (0.005)\tLoss 0.9625 (0.8822)\tPrec 64.844% (68.769%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.353 (0.353)\tLoss 0.8369 (0.8369)\tPrec 67.188% (67.188%)\n",
      " * Prec 64.900% \n",
      "best acc: 65.740000\n",
      "Epoch: [15][0/391]\tTime 0.570 (0.570)\tData 0.520 (0.520)\tLoss 0.7259 (0.7259)\tPrec 71.875% (71.875%)\n",
      "Epoch: [15][200/391]\tTime 0.040 (0.043)\tData 0.003 (0.005)\tLoss 0.8839 (0.8441)\tPrec 68.750% (70.091%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.525 (0.525)\tLoss 0.8118 (0.8118)\tPrec 74.219% (74.219%)\n",
      " * Prec 66.060% \n",
      "best acc: 66.060000\n",
      "Epoch: [16][0/391]\tTime 0.533 (0.533)\tData 0.483 (0.483)\tLoss 0.9271 (0.9271)\tPrec 64.844% (64.844%)\n",
      "Epoch: [16][200/391]\tTime 0.036 (0.043)\tData 0.002 (0.005)\tLoss 0.8672 (0.7955)\tPrec 65.625% (72.248%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.365 (0.365)\tLoss 0.8063 (0.8063)\tPrec 74.219% (74.219%)\n",
      " * Prec 70.780% \n",
      "best acc: 70.780000\n",
      "Epoch: [17][0/391]\tTime 0.475 (0.475)\tData 0.426 (0.426)\tLoss 0.8072 (0.8072)\tPrec 71.094% (71.094%)\n",
      "Epoch: [17][200/391]\tTime 0.041 (0.042)\tData 0.003 (0.005)\tLoss 0.7242 (0.7541)\tPrec 74.219% (74.024%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.317 (0.317)\tLoss 0.7795 (0.7795)\tPrec 72.656% (72.656%)\n",
      " * Prec 70.890% \n",
      "best acc: 70.890000\n",
      "Epoch: [18][0/391]\tTime 0.413 (0.413)\tData 0.365 (0.365)\tLoss 0.8277 (0.8277)\tPrec 71.094% (71.094%)\n",
      "Epoch: [18][200/391]\tTime 0.039 (0.041)\tData 0.002 (0.004)\tLoss 0.5844 (0.7194)\tPrec 81.250% (74.957%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.394 (0.394)\tLoss 0.7915 (0.7915)\tPrec 73.438% (73.438%)\n",
      " * Prec 71.180% \n",
      "best acc: 71.180000\n",
      "Epoch: [19][0/391]\tTime 0.571 (0.571)\tData 0.530 (0.530)\tLoss 0.8282 (0.8282)\tPrec 71.094% (71.094%)\n",
      "Epoch: [19][200/391]\tTime 0.044 (0.043)\tData 0.004 (0.005)\tLoss 0.6402 (0.6745)\tPrec 78.906% (76.936%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.302 (0.302)\tLoss 0.6889 (0.6889)\tPrec 76.562% (76.562%)\n",
      " * Prec 71.630% \n",
      "best acc: 71.630000\n",
      "Epoch: [20][0/391]\tTime 0.554 (0.554)\tData 0.499 (0.499)\tLoss 0.6405 (0.6405)\tPrec 78.125% (78.125%)\n",
      "Epoch: [20][200/391]\tTime 0.030 (0.042)\tData 0.002 (0.005)\tLoss 0.8771 (0.6418)\tPrec 71.875% (78.187%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.436 (0.436)\tLoss 0.8760 (0.8760)\tPrec 70.312% (70.312%)\n",
      " * Prec 72.990% \n",
      "best acc: 72.990000\n",
      "Epoch: [21][0/391]\tTime 0.649 (0.649)\tData 0.598 (0.598)\tLoss 0.5196 (0.5196)\tPrec 82.812% (82.812%)\n",
      "Epoch: [21][200/391]\tTime 0.041 (0.041)\tData 0.002 (0.006)\tLoss 0.6910 (0.6162)\tPrec 71.094% (78.972%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.448 (0.448)\tLoss 0.5200 (0.5200)\tPrec 85.938% (85.938%)\n",
      " * Prec 78.410% \n",
      "best acc: 78.410000\n",
      "Epoch: [22][0/391]\tTime 0.645 (0.645)\tData 0.601 (0.601)\tLoss 0.5631 (0.5631)\tPrec 79.688% (79.688%)\n",
      "Epoch: [22][200/391]\tTime 0.051 (0.045)\tData 0.004 (0.006)\tLoss 0.4579 (0.5867)\tPrec 82.812% (79.987%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.377 (0.377)\tLoss 0.6647 (0.6647)\tPrec 77.344% (77.344%)\n",
      " * Prec 74.700% \n",
      "best acc: 78.410000\n",
      "Epoch: [23][0/391]\tTime 0.534 (0.534)\tData 0.489 (0.489)\tLoss 0.4549 (0.4549)\tPrec 85.156% (85.156%)\n",
      "Epoch: [23][200/391]\tTime 0.031 (0.043)\tData 0.003 (0.005)\tLoss 0.5591 (0.5592)\tPrec 84.375% (80.865%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.424 (0.424)\tLoss 0.5744 (0.5744)\tPrec 81.250% (81.250%)\n",
      " * Prec 78.390% \n",
      "best acc: 78.410000\n",
      "Epoch: [24][0/391]\tTime 0.464 (0.464)\tData 0.419 (0.419)\tLoss 0.5233 (0.5233)\tPrec 82.812% (82.812%)\n",
      "Epoch: [24][200/391]\tTime 0.043 (0.042)\tData 0.003 (0.005)\tLoss 0.4303 (0.5357)\tPrec 84.375% (81.891%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.398 (0.398)\tLoss 0.5389 (0.5389)\tPrec 85.156% (85.156%)\n",
      " * Prec 79.610% \n",
      "best acc: 79.610000\n",
      "Epoch: [25][0/391]\tTime 0.443 (0.443)\tData 0.401 (0.401)\tLoss 0.5247 (0.5247)\tPrec 80.469% (80.469%)\n",
      "Epoch: [25][200/391]\tTime 0.046 (0.042)\tData 0.005 (0.005)\tLoss 0.5936 (0.4997)\tPrec 82.812% (83.088%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.294 (0.294)\tLoss 0.5024 (0.5024)\tPrec 85.156% (85.156%)\n",
      " * Prec 79.410% \n",
      "best acc: 79.610000\n",
      "Epoch: [26][0/391]\tTime 0.473 (0.473)\tData 0.427 (0.427)\tLoss 0.3701 (0.3701)\tPrec 88.281% (88.281%)\n",
      "Epoch: [26][200/391]\tTime 0.038 (0.042)\tData 0.003 (0.005)\tLoss 0.3658 (0.4946)\tPrec 87.500% (83.333%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.396 (0.396)\tLoss 0.5545 (0.5545)\tPrec 83.594% (83.594%)\n",
      " * Prec 79.530% \n",
      "best acc: 79.610000\n",
      "Epoch: [27][0/391]\tTime 0.515 (0.515)\tData 0.466 (0.466)\tLoss 0.5554 (0.5554)\tPrec 85.156% (85.156%)\n",
      "Epoch: [27][200/391]\tTime 0.039 (0.044)\tData 0.003 (0.005)\tLoss 0.3889 (0.4696)\tPrec 85.938% (84.192%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.524 (0.524)\tLoss 0.5077 (0.5077)\tPrec 82.031% (82.031%)\n",
      " * Prec 81.760% \n",
      "best acc: 81.760000\n",
      "Epoch: [28][0/391]\tTime 0.503 (0.503)\tData 0.453 (0.453)\tLoss 0.4576 (0.4576)\tPrec 82.031% (82.031%)\n",
      "Epoch: [28][200/391]\tTime 0.039 (0.042)\tData 0.002 (0.005)\tLoss 0.4715 (0.4497)\tPrec 82.031% (84.709%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.453 (0.453)\tLoss 0.5288 (0.5288)\tPrec 79.688% (79.688%)\n",
      " * Prec 80.720% \n",
      "best acc: 81.760000\n",
      "Epoch: [29][0/391]\tTime 0.516 (0.516)\tData 0.454 (0.454)\tLoss 0.3676 (0.3676)\tPrec 86.719% (86.719%)\n",
      "Epoch: [29][200/391]\tTime 0.038 (0.040)\tData 0.002 (0.005)\tLoss 0.4122 (0.4393)\tPrec 82.812% (85.032%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.422 (0.422)\tLoss 0.4440 (0.4440)\tPrec 86.719% (86.719%)\n",
      " * Prec 82.170% \n",
      "best acc: 82.170000\n",
      "Epoch: [30][0/391]\tTime 0.559 (0.559)\tData 0.507 (0.507)\tLoss 0.5838 (0.5838)\tPrec 81.250% (81.250%)\n",
      "Epoch: [30][200/391]\tTime 0.044 (0.043)\tData 0.003 (0.005)\tLoss 0.4359 (0.4165)\tPrec 83.594% (85.930%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.392 (0.392)\tLoss 0.5403 (0.5403)\tPrec 80.469% (80.469%)\n",
      " * Prec 81.550% \n",
      "best acc: 82.170000\n",
      "Epoch: [31][0/391]\tTime 0.537 (0.537)\tData 0.489 (0.489)\tLoss 0.3926 (0.3926)\tPrec 85.938% (85.938%)\n",
      "Epoch: [31][200/391]\tTime 0.040 (0.042)\tData 0.002 (0.005)\tLoss 0.3147 (0.4148)\tPrec 85.938% (85.988%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.358 (0.358)\tLoss 0.6158 (0.6158)\tPrec 78.906% (78.906%)\n",
      " * Prec 79.590% \n",
      "best acc: 82.170000\n",
      "Epoch: [32][0/391]\tTime 0.624 (0.624)\tData 0.578 (0.578)\tLoss 0.4798 (0.4798)\tPrec 83.594% (83.594%)\n",
      "Epoch: [32][200/391]\tTime 0.047 (0.043)\tData 0.004 (0.006)\tLoss 0.4000 (0.3991)\tPrec 86.719% (86.439%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.317 (0.317)\tLoss 0.5168 (0.5168)\tPrec 85.156% (85.156%)\n",
      " * Prec 81.520% \n",
      "best acc: 82.170000\n",
      "Epoch: [33][0/391]\tTime 0.540 (0.540)\tData 0.485 (0.485)\tLoss 0.3158 (0.3158)\tPrec 91.406% (91.406%)\n",
      "Epoch: [33][200/391]\tTime 0.047 (0.043)\tData 0.003 (0.005)\tLoss 0.4793 (0.3833)\tPrec 85.156% (86.925%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.277 (0.277)\tLoss 0.4586 (0.4586)\tPrec 86.719% (86.719%)\n",
      " * Prec 82.270% \n",
      "best acc: 82.270000\n",
      "Epoch: [34][0/391]\tTime 0.531 (0.531)\tData 0.478 (0.478)\tLoss 0.3532 (0.3532)\tPrec 89.062% (89.062%)\n",
      "Epoch: [34][200/391]\tTime 0.042 (0.043)\tData 0.003 (0.005)\tLoss 0.3824 (0.3670)\tPrec 86.719% (87.574%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.437 (0.437)\tLoss 0.5523 (0.5523)\tPrec 83.594% (83.594%)\n",
      " * Prec 82.200% \n",
      "best acc: 82.270000\n",
      "Epoch: [35][0/391]\tTime 0.508 (0.508)\tData 0.460 (0.460)\tLoss 0.3426 (0.3426)\tPrec 90.625% (90.625%)\n",
      "Epoch: [35][200/391]\tTime 0.041 (0.042)\tData 0.003 (0.005)\tLoss 0.3637 (0.3556)\tPrec 89.844% (87.889%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.289 (0.289)\tLoss 0.5687 (0.5687)\tPrec 78.125% (78.125%)\n",
      " * Prec 82.680% \n",
      "best acc: 82.680000\n",
      "Epoch: [36][0/391]\tTime 0.481 (0.481)\tData 0.433 (0.433)\tLoss 0.3625 (0.3625)\tPrec 86.719% (86.719%)\n",
      "Epoch: [36][200/391]\tTime 0.042 (0.043)\tData 0.003 (0.005)\tLoss 0.3967 (0.3441)\tPrec 86.719% (88.301%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.294 (0.294)\tLoss 0.5735 (0.5735)\tPrec 82.031% (82.031%)\n",
      " * Prec 83.400% \n",
      "best acc: 83.400000\n",
      "Epoch: [37][0/391]\tTime 0.506 (0.506)\tData 0.458 (0.458)\tLoss 0.4302 (0.4302)\tPrec 85.938% (85.938%)\n",
      "Epoch: [37][200/391]\tTime 0.044 (0.043)\tData 0.004 (0.005)\tLoss 0.4577 (0.3385)\tPrec 85.156% (88.472%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.418 (0.418)\tLoss 0.4343 (0.4343)\tPrec 85.156% (85.156%)\n",
      " * Prec 81.380% \n",
      "best acc: 83.400000\n",
      "Epoch: [38][0/391]\tTime 0.486 (0.486)\tData 0.440 (0.440)\tLoss 0.4274 (0.4274)\tPrec 86.719% (86.719%)\n",
      "Epoch: [38][200/391]\tTime 0.039 (0.043)\tData 0.002 (0.005)\tLoss 0.3090 (0.3265)\tPrec 89.062% (88.872%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.428 (0.428)\tLoss 0.3845 (0.3845)\tPrec 87.500% (87.500%)\n",
      " * Prec 84.360% \n",
      "best acc: 84.360000\n",
      "Epoch: [39][0/391]\tTime 0.461 (0.461)\tData 0.410 (0.410)\tLoss 0.3467 (0.3467)\tPrec 89.062% (89.062%)\n",
      "Epoch: [39][200/391]\tTime 0.043 (0.043)\tData 0.002 (0.005)\tLoss 0.2252 (0.3074)\tPrec 92.188% (89.591%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.316 (0.316)\tLoss 0.5304 (0.5304)\tPrec 81.250% (81.250%)\n",
      " * Prec 82.850% \n",
      "best acc: 84.360000\n",
      "Epoch: [40][0/391]\tTime 0.522 (0.522)\tData 0.484 (0.484)\tLoss 0.3114 (0.3114)\tPrec 89.844% (89.844%)\n",
      "Epoch: [40][200/391]\tTime 0.040 (0.043)\tData 0.002 (0.005)\tLoss 0.2863 (0.3148)\tPrec 88.281% (89.401%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.434 (0.434)\tLoss 0.4015 (0.4015)\tPrec 85.938% (85.938%)\n",
      " * Prec 84.970% \n",
      "best acc: 84.970000\n",
      "Epoch: [41][0/391]\tTime 0.549 (0.549)\tData 0.501 (0.501)\tLoss 0.2965 (0.2965)\tPrec 90.625% (90.625%)\n",
      "Epoch: [41][200/391]\tTime 0.038 (0.043)\tData 0.003 (0.005)\tLoss 0.2816 (0.2970)\tPrec 89.844% (89.960%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.565 (0.565)\tLoss 0.3409 (0.3409)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.100% \n",
      "best acc: 85.100000\n",
      "Epoch: [42][0/391]\tTime 0.614 (0.614)\tData 0.558 (0.558)\tLoss 0.2782 (0.2782)\tPrec 89.062% (89.062%)\n",
      "Epoch: [42][200/391]\tTime 0.038 (0.043)\tData 0.002 (0.005)\tLoss 0.2284 (0.2961)\tPrec 91.406% (89.875%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.371 (0.371)\tLoss 0.3301 (0.3301)\tPrec 89.844% (89.844%)\n",
      " * Prec 84.810% \n",
      "best acc: 85.100000\n",
      "Epoch: [43][0/391]\tTime 0.481 (0.481)\tData 0.432 (0.432)\tLoss 0.2649 (0.2649)\tPrec 92.188% (92.188%)\n",
      "Epoch: [43][200/391]\tTime 0.036 (0.039)\tData 0.003 (0.005)\tLoss 0.3347 (0.2818)\tPrec 89.062% (90.438%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.400 (0.400)\tLoss 0.3340 (0.3340)\tPrec 89.062% (89.062%)\n",
      " * Prec 84.310% \n",
      "best acc: 85.100000\n",
      "Epoch: [44][0/391]\tTime 0.520 (0.520)\tData 0.472 (0.472)\tLoss 0.3155 (0.3155)\tPrec 89.844% (89.844%)\n",
      "Epoch: [44][200/391]\tTime 0.041 (0.043)\tData 0.004 (0.005)\tLoss 0.3055 (0.2770)\tPrec 87.500% (90.481%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.356 (0.356)\tLoss 0.6632 (0.6632)\tPrec 76.562% (76.562%)\n",
      " * Prec 80.290% \n",
      "best acc: 85.100000\n",
      "Epoch: [45][0/391]\tTime 0.572 (0.572)\tData 0.524 (0.524)\tLoss 0.3516 (0.3516)\tPrec 88.281% (88.281%)\n",
      "Epoch: [45][200/391]\tTime 0.038 (0.043)\tData 0.002 (0.005)\tLoss 0.1816 (0.2674)\tPrec 91.406% (91.029%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.312 (0.312)\tLoss 0.3323 (0.3323)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.170% \n",
      "best acc: 86.170000\n",
      "Epoch: [46][0/391]\tTime 0.440 (0.440)\tData 0.389 (0.389)\tLoss 0.3155 (0.3155)\tPrec 89.062% (89.062%)\n",
      "Epoch: [46][200/391]\tTime 0.036 (0.042)\tData 0.003 (0.005)\tLoss 0.2211 (0.2535)\tPrec 91.406% (91.348%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.370 (0.370)\tLoss 0.3503 (0.3503)\tPrec 87.500% (87.500%)\n",
      " * Prec 84.720% \n",
      "best acc: 86.170000\n",
      "Epoch: [47][0/391]\tTime 0.515 (0.515)\tData 0.461 (0.461)\tLoss 0.2237 (0.2237)\tPrec 91.406% (91.406%)\n",
      "Epoch: [47][200/391]\tTime 0.035 (0.042)\tData 0.002 (0.005)\tLoss 0.2295 (0.2475)\tPrec 92.969% (91.647%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.411 (0.411)\tLoss 0.3081 (0.3081)\tPrec 92.188% (92.188%)\n",
      " * Prec 85.500% \n",
      "best acc: 86.170000\n",
      "Epoch: [48][0/391]\tTime 0.577 (0.577)\tData 0.534 (0.534)\tLoss 0.2914 (0.2914)\tPrec 89.844% (89.844%)\n",
      "Epoch: [48][200/391]\tTime 0.040 (0.036)\tData 0.003 (0.005)\tLoss 0.2273 (0.2454)\tPrec 90.625% (91.430%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.430 (0.430)\tLoss 0.4285 (0.4285)\tPrec 85.938% (85.938%)\n",
      " * Prec 84.650% \n",
      "best acc: 86.170000\n",
      "Epoch: [49][0/391]\tTime 0.507 (0.507)\tData 0.467 (0.467)\tLoss 0.3311 (0.3311)\tPrec 89.844% (89.844%)\n",
      "Epoch: [49][200/391]\tTime 0.041 (0.042)\tData 0.002 (0.005)\tLoss 0.1196 (0.2355)\tPrec 96.094% (91.900%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.414 (0.414)\tLoss 0.4163 (0.4163)\tPrec 84.375% (84.375%)\n",
      " * Prec 83.820% \n",
      "best acc: 86.170000\n",
      "Epoch: [50][0/391]\tTime 0.608 (0.608)\tData 0.569 (0.569)\tLoss 0.2019 (0.2019)\tPrec 93.750% (93.750%)\n",
      "Epoch: [50][200/391]\tTime 0.039 (0.044)\tData 0.003 (0.006)\tLoss 0.2221 (0.2277)\tPrec 89.844% (92.160%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.393 (0.393)\tLoss 0.2273 (0.2273)\tPrec 92.188% (92.188%)\n",
      " * Prec 85.740% \n",
      "best acc: 86.170000\n",
      "Epoch: [51][0/391]\tTime 0.560 (0.560)\tData 0.521 (0.521)\tLoss 0.3207 (0.3207)\tPrec 87.500% (87.500%)\n",
      "Epoch: [51][200/391]\tTime 0.027 (0.041)\tData 0.002 (0.005)\tLoss 0.2522 (0.2258)\tPrec 89.844% (92.226%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.388 (0.388)\tLoss 0.2768 (0.2768)\tPrec 92.188% (92.188%)\n",
      " * Prec 86.840% \n",
      "best acc: 86.840000\n",
      "Epoch: [52][0/391]\tTime 0.520 (0.520)\tData 0.478 (0.478)\tLoss 0.1857 (0.1857)\tPrec 93.750% (93.750%)\n",
      "Epoch: [52][200/391]\tTime 0.042 (0.039)\tData 0.003 (0.005)\tLoss 0.1850 (0.2225)\tPrec 94.531% (92.471%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.455 (0.455)\tLoss 0.3144 (0.3144)\tPrec 90.625% (90.625%)\n",
      " * Prec 86.470% \n",
      "best acc: 86.840000\n",
      "Epoch: [53][0/391]\tTime 0.482 (0.482)\tData 0.433 (0.433)\tLoss 0.1934 (0.1934)\tPrec 92.188% (92.188%)\n",
      "Epoch: [53][200/391]\tTime 0.035 (0.042)\tData 0.003 (0.005)\tLoss 0.2010 (0.2163)\tPrec 92.969% (92.638%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.377 (0.377)\tLoss 0.3600 (0.3600)\tPrec 85.156% (85.156%)\n",
      " * Prec 86.000% \n",
      "best acc: 86.840000\n",
      "Epoch: [54][0/391]\tTime 0.493 (0.493)\tData 0.450 (0.450)\tLoss 0.1330 (0.1330)\tPrec 95.312% (95.312%)\n",
      "Epoch: [54][200/391]\tTime 0.044 (0.043)\tData 0.003 (0.005)\tLoss 0.1534 (0.2094)\tPrec 93.750% (92.895%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.506 (0.506)\tLoss 0.3279 (0.3279)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.980% \n",
      "best acc: 86.980000\n",
      "Epoch: [55][0/391]\tTime 0.499 (0.499)\tData 0.463 (0.463)\tLoss 0.1196 (0.1196)\tPrec 94.531% (94.531%)\n",
      "Epoch: [55][200/391]\tTime 0.042 (0.038)\tData 0.002 (0.005)\tLoss 0.1635 (0.2044)\tPrec 95.312% (92.910%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.330 (0.330)\tLoss 0.2927 (0.2927)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.830% \n",
      "best acc: 86.980000\n",
      "Epoch: [56][0/391]\tTime 0.501 (0.501)\tData 0.450 (0.450)\tLoss 0.1183 (0.1183)\tPrec 96.094% (96.094%)\n",
      "Epoch: [56][200/391]\tTime 0.034 (0.043)\tData 0.003 (0.005)\tLoss 0.2299 (0.1968)\tPrec 93.750% (93.268%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.429 (0.429)\tLoss 0.3761 (0.3761)\tPrec 90.625% (90.625%)\n",
      " * Prec 86.600% \n",
      "best acc: 86.980000\n",
      "Epoch: [57][0/391]\tTime 0.585 (0.585)\tData 0.534 (0.534)\tLoss 0.2192 (0.2192)\tPrec 91.406% (91.406%)\n",
      "Epoch: [57][200/391]\tTime 0.041 (0.043)\tData 0.003 (0.005)\tLoss 0.1597 (0.1961)\tPrec 95.312% (93.229%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.404 (0.404)\tLoss 0.2637 (0.2637)\tPrec 93.750% (93.750%)\n",
      " * Prec 87.160% \n",
      "best acc: 87.160000\n",
      "Epoch: [58][0/391]\tTime 0.627 (0.627)\tData 0.575 (0.575)\tLoss 0.2521 (0.2521)\tPrec 91.406% (91.406%)\n",
      "Epoch: [58][200/391]\tTime 0.040 (0.044)\tData 0.002 (0.005)\tLoss 0.1929 (0.1903)\tPrec 92.188% (93.497%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.417 (0.417)\tLoss 0.3606 (0.3606)\tPrec 87.500% (87.500%)\n",
      " * Prec 84.490% \n",
      "best acc: 87.160000\n",
      "Epoch: [59][0/391]\tTime 0.468 (0.468)\tData 0.427 (0.427)\tLoss 0.1084 (0.1084)\tPrec 97.656% (97.656%)\n",
      "Epoch: [59][200/391]\tTime 0.033 (0.041)\tData 0.002 (0.004)\tLoss 0.2724 (0.1875)\tPrec 91.406% (93.478%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.512 (0.512)\tLoss 0.3189 (0.3189)\tPrec 90.625% (90.625%)\n",
      " * Prec 86.770% \n",
      "best acc: 87.160000\n",
      "Epoch: [60][0/391]\tTime 0.783 (0.783)\tData 0.734 (0.734)\tLoss 0.1285 (0.1285)\tPrec 94.531% (94.531%)\n",
      "Epoch: [60][200/391]\tTime 0.043 (0.044)\tData 0.003 (0.006)\tLoss 0.2254 (0.1854)\tPrec 92.188% (93.738%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.457 (0.457)\tLoss 0.2562 (0.2562)\tPrec 92.188% (92.188%)\n",
      " * Prec 86.550% \n",
      "best acc: 87.160000\n",
      "Epoch: [61][0/391]\tTime 0.526 (0.526)\tData 0.477 (0.477)\tLoss 0.1306 (0.1306)\tPrec 95.312% (95.312%)\n",
      "Epoch: [61][200/391]\tTime 0.040 (0.043)\tData 0.002 (0.005)\tLoss 0.1729 (0.1702)\tPrec 92.969% (94.115%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.209 (0.209)\tLoss 0.2673 (0.2673)\tPrec 92.188% (92.188%)\n",
      " * Prec 85.700% \n",
      "best acc: 87.160000\n",
      "Epoch: [62][0/391]\tTime 0.658 (0.658)\tData 0.609 (0.609)\tLoss 0.2167 (0.2167)\tPrec 92.188% (92.188%)\n",
      "Epoch: [62][200/391]\tTime 0.040 (0.043)\tData 0.002 (0.005)\tLoss 0.0781 (0.1731)\tPrec 98.438% (93.975%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.207 (0.207)\tLoss 0.3503 (0.3503)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.960% \n",
      "best acc: 87.160000\n",
      "Epoch: [63][0/391]\tTime 0.472 (0.472)\tData 0.424 (0.424)\tLoss 0.1955 (0.1955)\tPrec 92.969% (92.969%)\n",
      "Epoch: [63][200/391]\tTime 0.041 (0.043)\tData 0.003 (0.005)\tLoss 0.1354 (0.1676)\tPrec 96.094% (94.220%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.428 (0.428)\tLoss 0.2509 (0.2509)\tPrec 90.625% (90.625%)\n",
      " * Prec 87.410% \n",
      "best acc: 87.410000\n",
      "Epoch: [64][0/391]\tTime 0.422 (0.422)\tData 0.373 (0.373)\tLoss 0.2006 (0.2006)\tPrec 92.188% (92.188%)\n",
      "Epoch: [64][200/391]\tTime 0.042 (0.042)\tData 0.003 (0.004)\tLoss 0.1613 (0.1596)\tPrec 92.969% (94.395%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.223 (0.223)\tLoss 0.2337 (0.2337)\tPrec 92.969% (92.969%)\n",
      " * Prec 87.530% \n",
      "best acc: 87.530000\n",
      "Epoch: [65][0/391]\tTime 0.408 (0.408)\tData 0.359 (0.359)\tLoss 0.1773 (0.1773)\tPrec 94.531% (94.531%)\n",
      "Epoch: [65][200/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.2300 (0.1628)\tPrec 93.750% (94.488%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.401 (0.401)\tLoss 0.2209 (0.2209)\tPrec 91.406% (91.406%)\n",
      " * Prec 86.320% \n",
      "best acc: 87.530000\n",
      "Epoch: [66][0/391]\tTime 0.607 (0.607)\tData 0.558 (0.558)\tLoss 0.1476 (0.1476)\tPrec 95.312% (95.312%)\n",
      "Epoch: [66][200/391]\tTime 0.043 (0.045)\tData 0.003 (0.005)\tLoss 0.2668 (0.1605)\tPrec 92.969% (94.597%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.508 (0.508)\tLoss 0.3194 (0.3194)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.970% \n",
      "best acc: 87.530000\n",
      "Epoch: [67][0/391]\tTime 0.634 (0.634)\tData 0.587 (0.587)\tLoss 0.1975 (0.1975)\tPrec 92.188% (92.188%)\n",
      "Epoch: [67][200/391]\tTime 0.040 (0.043)\tData 0.002 (0.005)\tLoss 0.1639 (0.1540)\tPrec 92.188% (94.799%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.326 (0.326)\tLoss 0.2811 (0.2811)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.620% \n",
      "best acc: 87.530000\n",
      "Epoch: [68][0/391]\tTime 0.460 (0.460)\tData 0.411 (0.411)\tLoss 0.1256 (0.1256)\tPrec 96.094% (96.094%)\n",
      "Epoch: [68][200/391]\tTime 0.047 (0.042)\tData 0.003 (0.004)\tLoss 0.1291 (0.1588)\tPrec 96.875% (94.481%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.209 (0.209)\tLoss 0.2878 (0.2878)\tPrec 92.188% (92.188%)\n",
      " * Prec 87.630% \n",
      "best acc: 87.630000\n",
      "Epoch: [69][0/391]\tTime 0.546 (0.546)\tData 0.496 (0.496)\tLoss 0.1311 (0.1311)\tPrec 94.531% (94.531%)\n",
      "Epoch: [69][200/391]\tTime 0.042 (0.043)\tData 0.003 (0.005)\tLoss 0.2436 (0.1618)\tPrec 94.531% (94.430%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.261 (0.261)\tLoss 0.3757 (0.3757)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.890% \n",
      "best acc: 87.630000\n",
      "Epoch: [70][0/391]\tTime 0.577 (0.577)\tData 0.530 (0.530)\tLoss 0.1717 (0.1717)\tPrec 92.188% (92.188%)\n",
      "Epoch: [70][200/391]\tTime 0.041 (0.043)\tData 0.002 (0.005)\tLoss 0.0771 (0.0955)\tPrec 97.656% (96.789%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.402 (0.402)\tLoss 0.1569 (0.1569)\tPrec 94.531% (94.531%)\n",
      " * Prec 90.170% \n",
      "best acc: 90.170000\n",
      "Epoch: [71][0/391]\tTime 0.570 (0.570)\tData 0.523 (0.523)\tLoss 0.0335 (0.0335)\tPrec 98.438% (98.438%)\n",
      "Epoch: [71][200/391]\tTime 0.040 (0.044)\tData 0.002 (0.005)\tLoss 0.0785 (0.0680)\tPrec 97.656% (97.703%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.297 (0.297)\tLoss 0.1509 (0.1509)\tPrec 94.531% (94.531%)\n",
      " * Prec 90.310% \n",
      "best acc: 90.310000\n",
      "Epoch: [72][0/391]\tTime 0.583 (0.583)\tData 0.535 (0.535)\tLoss 0.0157 (0.0157)\tPrec 100.000% (100.000%)\n",
      "Epoch: [72][200/391]\tTime 0.040 (0.043)\tData 0.002 (0.005)\tLoss 0.0326 (0.0604)\tPrec 99.219% (98.057%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 0.1244 (0.1244)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.460% \n",
      "best acc: 90.460000\n",
      "Epoch: [73][0/391]\tTime 0.563 (0.563)\tData 0.511 (0.511)\tLoss 0.0762 (0.0762)\tPrec 96.875% (96.875%)\n",
      "Epoch: [73][200/391]\tTime 0.039 (0.043)\tData 0.002 (0.005)\tLoss 0.0491 (0.0516)\tPrec 98.438% (98.344%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.527 (0.527)\tLoss 0.1247 (0.1247)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.510% \n",
      "best acc: 90.510000\n",
      "Epoch: [74][0/391]\tTime 0.555 (0.555)\tData 0.507 (0.507)\tLoss 0.0159 (0.0159)\tPrec 99.219% (99.219%)\n",
      "Epoch: [74][200/391]\tTime 0.041 (0.043)\tData 0.002 (0.005)\tLoss 0.0734 (0.0506)\tPrec 97.656% (98.298%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.216 (0.216)\tLoss 0.1506 (0.1506)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.440% \n",
      "best acc: 90.510000\n",
      "Epoch: [75][0/391]\tTime 0.455 (0.455)\tData 0.403 (0.403)\tLoss 0.0514 (0.0514)\tPrec 97.656% (97.656%)\n",
      "Epoch: [75][200/391]\tTime 0.032 (0.042)\tData 0.001 (0.004)\tLoss 0.0258 (0.0424)\tPrec 99.219% (98.570%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.311 (0.311)\tLoss 0.1210 (0.1210)\tPrec 96.875% (96.875%)\n",
      " * Prec 90.730% \n",
      "best acc: 90.730000\n",
      "Epoch: [76][0/391]\tTime 0.451 (0.451)\tData 0.393 (0.393)\tLoss 0.0643 (0.0643)\tPrec 96.094% (96.094%)\n",
      "Epoch: [76][200/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.0533 (0.0414)\tPrec 96.875% (98.597%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.301 (0.301)\tLoss 0.1309 (0.1309)\tPrec 94.531% (94.531%)\n",
      " * Prec 90.710% \n",
      "best acc: 90.730000\n",
      "Epoch: [77][0/391]\tTime 0.569 (0.569)\tData 0.535 (0.535)\tLoss 0.0273 (0.0273)\tPrec 98.438% (98.438%)\n",
      "Epoch: [77][200/391]\tTime 0.040 (0.043)\tData 0.002 (0.005)\tLoss 0.0115 (0.0433)\tPrec 100.000% (98.500%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.352 (0.352)\tLoss 0.1162 (0.1162)\tPrec 98.438% (98.438%)\n",
      " * Prec 90.530% \n",
      "best acc: 90.730000\n",
      "Epoch: [78][0/391]\tTime 0.504 (0.504)\tData 0.450 (0.450)\tLoss 0.0159 (0.0159)\tPrec 100.000% (100.000%)\n",
      "Epoch: [78][200/391]\tTime 0.041 (0.042)\tData 0.001 (0.004)\tLoss 0.0132 (0.0426)\tPrec 100.000% (98.651%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 0.1474 (0.1474)\tPrec 94.531% (94.531%)\n",
      " * Prec 90.270% \n",
      "best acc: 90.730000\n",
      "Epoch: [79][0/391]\tTime 0.587 (0.587)\tData 0.548 (0.548)\tLoss 0.0372 (0.0372)\tPrec 98.438% (98.438%)\n",
      "Epoch: [79][200/391]\tTime 0.043 (0.043)\tData 0.003 (0.005)\tLoss 0.0226 (0.0316)\tPrec 99.219% (98.919%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.380 (0.380)\tLoss 0.1357 (0.1357)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.370% \n",
      "best acc: 90.730000\n",
      "Epoch: [80][0/391]\tTime 0.549 (0.549)\tData 0.480 (0.480)\tLoss 0.0417 (0.0417)\tPrec 98.438% (98.438%)\n",
      "Epoch: [80][200/391]\tTime 0.041 (0.043)\tData 0.003 (0.005)\tLoss 0.0133 (0.0297)\tPrec 100.000% (98.978%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.453 (0.453)\tLoss 0.1186 (0.1186)\tPrec 98.438% (98.438%)\n",
      " * Prec 90.380% \n",
      "best acc: 90.730000\n",
      "Epoch: [81][0/391]\tTime 0.679 (0.679)\tData 0.629 (0.629)\tLoss 0.0097 (0.0097)\tPrec 100.000% (100.000%)\n",
      "Epoch: [81][200/391]\tTime 0.040 (0.043)\tData 0.002 (0.006)\tLoss 0.0640 (0.0350)\tPrec 96.094% (98.733%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.490 (0.490)\tLoss 0.1550 (0.1550)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.720% \n",
      "best acc: 90.730000\n",
      "Epoch: [82][0/391]\tTime 0.473 (0.473)\tData 0.422 (0.422)\tLoss 0.0073 (0.0073)\tPrec 100.000% (100.000%)\n",
      "Epoch: [82][200/391]\tTime 0.042 (0.043)\tData 0.002 (0.005)\tLoss 0.0133 (0.0308)\tPrec 100.000% (99.017%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.392 (0.392)\tLoss 0.1264 (0.1264)\tPrec 96.875% (96.875%)\n",
      " * Prec 90.510% \n",
      "best acc: 90.730000\n",
      "Epoch: [83][0/391]\tTime 0.676 (0.676)\tData 0.632 (0.632)\tLoss 0.0518 (0.0518)\tPrec 98.438% (98.438%)\n",
      "Epoch: [83][200/391]\tTime 0.030 (0.044)\tData 0.002 (0.006)\tLoss 0.0514 (0.0281)\tPrec 98.438% (99.009%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.384 (0.384)\tLoss 0.1443 (0.1443)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.340% \n",
      "best acc: 90.730000\n",
      "Epoch: [84][0/391]\tTime 0.396 (0.396)\tData 0.348 (0.348)\tLoss 0.0668 (0.0668)\tPrec 97.656% (97.656%)\n",
      "Epoch: [84][200/391]\tTime 0.040 (0.042)\tData 0.003 (0.004)\tLoss 0.0649 (0.0305)\tPrec 97.656% (98.993%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.536 (0.536)\tLoss 0.1200 (0.1200)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.460% \n",
      "best acc: 90.730000\n",
      "Epoch: [85][0/391]\tTime 0.659 (0.659)\tData 0.610 (0.610)\tLoss 0.0259 (0.0259)\tPrec 99.219% (99.219%)\n",
      "Epoch: [85][200/391]\tTime 0.038 (0.043)\tData 0.003 (0.006)\tLoss 0.0149 (0.0275)\tPrec 99.219% (99.052%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.326 (0.326)\tLoss 0.1118 (0.1118)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.580% \n",
      "best acc: 90.730000\n",
      "Epoch: [86][0/391]\tTime 0.491 (0.491)\tData 0.442 (0.442)\tLoss 0.0406 (0.0406)\tPrec 97.656% (97.656%)\n",
      "Epoch: [86][200/391]\tTime 0.042 (0.043)\tData 0.002 (0.004)\tLoss 0.0094 (0.0310)\tPrec 100.000% (99.005%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.523 (0.523)\tLoss 0.1340 (0.1340)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.410% \n",
      "best acc: 90.730000\n",
      "Epoch: [87][0/391]\tTime 0.461 (0.461)\tData 0.406 (0.406)\tLoss 0.0332 (0.0332)\tPrec 99.219% (99.219%)\n",
      "Epoch: [87][200/391]\tTime 0.040 (0.042)\tData 0.002 (0.004)\tLoss 0.0131 (0.0263)\tPrec 99.219% (99.087%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.502 (0.502)\tLoss 0.0858 (0.0858)\tPrec 96.875% (96.875%)\n",
      " * Prec 90.560% \n",
      "best acc: 90.730000\n",
      "Epoch: [88][0/391]\tTime 0.581 (0.581)\tData 0.533 (0.533)\tLoss 0.0528 (0.0528)\tPrec 98.438% (98.438%)\n",
      "Epoch: [88][200/391]\tTime 0.041 (0.043)\tData 0.002 (0.005)\tLoss 0.0388 (0.0272)\tPrec 97.656% (99.032%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.502 (0.502)\tLoss 0.1030 (0.1030)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.530% \n",
      "best acc: 90.730000\n",
      "Epoch: [89][0/391]\tTime 0.699 (0.699)\tData 0.652 (0.652)\tLoss 0.0510 (0.0510)\tPrec 98.438% (98.438%)\n",
      "Epoch: [89][200/391]\tTime 0.042 (0.044)\tData 0.002 (0.006)\tLoss 0.0377 (0.0244)\tPrec 98.438% (99.137%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.291 (0.291)\tLoss 0.1315 (0.1315)\tPrec 94.531% (94.531%)\n",
      " * Prec 90.630% \n",
      "best acc: 90.730000\n",
      "Epoch: [90][0/391]\tTime 0.591 (0.591)\tData 0.524 (0.524)\tLoss 0.0101 (0.0101)\tPrec 100.000% (100.000%)\n",
      "Epoch: [90][200/391]\tTime 0.039 (0.043)\tData 0.002 (0.005)\tLoss 0.0059 (0.0232)\tPrec 100.000% (99.242%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.207 (0.207)\tLoss 0.1355 (0.1355)\tPrec 94.531% (94.531%)\n",
      " * Prec 90.650% \n",
      "best acc: 90.730000\n",
      "Epoch: [91][0/391]\tTime 0.415 (0.415)\tData 0.367 (0.367)\tLoss 0.0449 (0.0449)\tPrec 98.438% (98.438%)\n",
      "Epoch: [91][200/391]\tTime 0.036 (0.042)\tData 0.003 (0.004)\tLoss 0.0138 (0.0210)\tPrec 100.000% (99.246%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.409 (0.409)\tLoss 0.1137 (0.1137)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.650% \n",
      "best acc: 90.730000\n",
      "Epoch: [92][0/391]\tTime 0.479 (0.479)\tData 0.428 (0.428)\tLoss 0.0071 (0.0071)\tPrec 100.000% (100.000%)\n",
      "Epoch: [92][200/391]\tTime 0.039 (0.043)\tData 0.002 (0.005)\tLoss 0.0060 (0.0212)\tPrec 100.000% (99.312%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.369 (0.369)\tLoss 0.1452 (0.1452)\tPrec 94.531% (94.531%)\n",
      " * Prec 90.670% \n",
      "best acc: 90.730000\n",
      "Epoch: [93][0/391]\tTime 0.481 (0.481)\tData 0.445 (0.445)\tLoss 0.0513 (0.0513)\tPrec 98.438% (98.438%)\n",
      "Epoch: [93][200/391]\tTime 0.038 (0.042)\tData 0.002 (0.005)\tLoss 0.0095 (0.0205)\tPrec 100.000% (99.359%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.382 (0.382)\tLoss 0.1173 (0.1173)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.670% \n",
      "best acc: 90.730000\n",
      "Epoch: [94][0/391]\tTime 0.520 (0.520)\tData 0.473 (0.473)\tLoss 0.0097 (0.0097)\tPrec 99.219% (99.219%)\n",
      "Epoch: [94][200/391]\tTime 0.040 (0.043)\tData 0.003 (0.005)\tLoss 0.0586 (0.0206)\tPrec 97.656% (99.304%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.204 (0.204)\tLoss 0.0817 (0.0817)\tPrec 96.875% (96.875%)\n",
      " * Prec 90.620% \n",
      "best acc: 90.730000\n",
      "Epoch: [95][0/391]\tTime 0.562 (0.562)\tData 0.509 (0.509)\tLoss 0.0413 (0.0413)\tPrec 98.438% (98.438%)\n",
      "Epoch: [95][200/391]\tTime 0.041 (0.043)\tData 0.003 (0.005)\tLoss 0.0081 (0.0200)\tPrec 100.000% (99.413%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.320 (0.320)\tLoss 0.1436 (0.1436)\tPrec 94.531% (94.531%)\n",
      " * Prec 90.490% \n",
      "best acc: 90.730000\n",
      "Epoch: [96][0/391]\tTime 0.515 (0.515)\tData 0.465 (0.465)\tLoss 0.0221 (0.0221)\tPrec 99.219% (99.219%)\n",
      "Epoch: [96][200/391]\tTime 0.040 (0.043)\tData 0.003 (0.005)\tLoss 0.0094 (0.0171)\tPrec 100.000% (99.506%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.319 (0.319)\tLoss 0.1167 (0.1167)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.700% \n",
      "best acc: 90.730000\n",
      "Epoch: [97][0/391]\tTime 0.467 (0.467)\tData 0.419 (0.419)\tLoss 0.0132 (0.0132)\tPrec 100.000% (100.000%)\n",
      "Epoch: [97][200/391]\tTime 0.046 (0.040)\tData 0.002 (0.005)\tLoss 0.0035 (0.0193)\tPrec 100.000% (99.405%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.437 (0.437)\tLoss 0.1366 (0.1366)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.630% \n",
      "best acc: 90.730000\n",
      "Epoch: [98][0/391]\tTime 0.671 (0.671)\tData 0.624 (0.624)\tLoss 0.0171 (0.0171)\tPrec 100.000% (100.000%)\n",
      "Epoch: [98][200/391]\tTime 0.040 (0.044)\tData 0.002 (0.005)\tLoss 0.0177 (0.0209)\tPrec 99.219% (99.293%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.526 (0.526)\tLoss 0.1057 (0.1057)\tPrec 96.875% (96.875%)\n",
      " * Prec 90.630% \n",
      "best acc: 90.730000\n",
      "Epoch: [99][0/391]\tTime 0.669 (0.669)\tData 0.621 (0.621)\tLoss 0.0347 (0.0347)\tPrec 98.438% (98.438%)\n",
      "Epoch: [99][200/391]\tTime 0.041 (0.043)\tData 0.002 (0.006)\tLoss 0.0204 (0.0205)\tPrec 99.219% (99.312%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.405 (0.405)\tLoss 0.1310 (0.1310)\tPrec 96.875% (96.875%)\n",
      " * Prec 90.640% \n",
      "best acc: 90.730000\n",
      "Epoch: [100][0/391]\tTime 0.439 (0.439)\tData 0.388 (0.388)\tLoss 0.1101 (0.1101)\tPrec 97.656% (97.656%)\n",
      "Epoch: [100][200/391]\tTime 0.040 (0.042)\tData 0.002 (0.004)\tLoss 0.0360 (0.0198)\tPrec 98.438% (99.300%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.281 (0.281)\tLoss 0.1268 (0.1268)\tPrec 96.875% (96.875%)\n",
      " * Prec 90.640% \n",
      "best acc: 90.730000\n",
      "Epoch: [101][0/391]\tTime 0.450 (0.450)\tData 0.398 (0.398)\tLoss 0.0342 (0.0342)\tPrec 99.219% (99.219%)\n",
      "Epoch: [101][200/391]\tTime 0.039 (0.042)\tData 0.003 (0.004)\tLoss 0.0141 (0.0186)\tPrec 100.000% (99.343%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.322 (0.322)\tLoss 0.1236 (0.1236)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.580% \n",
      "best acc: 90.730000\n",
      "Epoch: [102][0/391]\tTime 0.420 (0.420)\tData 0.370 (0.370)\tLoss 0.0344 (0.0344)\tPrec 99.219% (99.219%)\n",
      "Epoch: [102][200/391]\tTime 0.040 (0.043)\tData 0.003 (0.004)\tLoss 0.0021 (0.0185)\tPrec 100.000% (99.394%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.445 (0.445)\tLoss 0.0957 (0.0957)\tPrec 96.875% (96.875%)\n",
      " * Prec 90.670% \n",
      "best acc: 90.730000\n",
      "Epoch: [103][0/391]\tTime 0.581 (0.581)\tData 0.533 (0.533)\tLoss 0.0058 (0.0058)\tPrec 100.000% (100.000%)\n",
      "Epoch: [103][200/391]\tTime 0.040 (0.043)\tData 0.002 (0.005)\tLoss 0.0256 (0.0175)\tPrec 99.219% (99.464%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.543 (0.543)\tLoss 0.1300 (0.1300)\tPrec 96.875% (96.875%)\n",
      " * Prec 90.770% \n",
      "best acc: 90.770000\n",
      "Epoch: [104][0/391]\tTime 0.567 (0.567)\tData 0.518 (0.518)\tLoss 0.0207 (0.0207)\tPrec 99.219% (99.219%)\n",
      "Epoch: [104][200/391]\tTime 0.041 (0.042)\tData 0.002 (0.005)\tLoss 0.0066 (0.0185)\tPrec 100.000% (99.401%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.196 (0.196)\tLoss 0.1017 (0.1017)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.610% \n",
      "best acc: 90.770000\n",
      "Epoch: [105][0/391]\tTime 0.520 (0.520)\tData 0.471 (0.471)\tLoss 0.0238 (0.0238)\tPrec 98.438% (98.438%)\n",
      "Epoch: [105][200/391]\tTime 0.041 (0.042)\tData 0.002 (0.005)\tLoss 0.0147 (0.0173)\tPrec 99.219% (99.452%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.483 (0.483)\tLoss 0.0994 (0.0994)\tPrec 97.656% (97.656%)\n",
      " * Prec 90.710% \n",
      "best acc: 90.770000\n",
      "Epoch: [106][0/391]\tTime 0.432 (0.432)\tData 0.385 (0.385)\tLoss 0.0234 (0.0234)\tPrec 99.219% (99.219%)\n",
      "Epoch: [106][200/391]\tTime 0.039 (0.042)\tData 0.002 (0.004)\tLoss 0.0042 (0.0204)\tPrec 100.000% (99.312%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.204 (0.204)\tLoss 0.0847 (0.0847)\tPrec 96.875% (96.875%)\n",
      " * Prec 90.690% \n",
      "best acc: 90.770000\n",
      "Epoch: [107][0/391]\tTime 0.474 (0.474)\tData 0.422 (0.422)\tLoss 0.0028 (0.0028)\tPrec 100.000% (100.000%)\n",
      "Epoch: [107][200/391]\tTime 0.036 (0.041)\tData 0.002 (0.005)\tLoss 0.0070 (0.0170)\tPrec 100.000% (99.436%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.447 (0.447)\tLoss 0.1402 (0.1402)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.580% \n",
      "best acc: 90.770000\n",
      "Epoch: [108][0/391]\tTime 0.543 (0.543)\tData 0.491 (0.491)\tLoss 0.0069 (0.0069)\tPrec 100.000% (100.000%)\n",
      "Epoch: [108][200/391]\tTime 0.041 (0.042)\tData 0.003 (0.005)\tLoss 0.0072 (0.0192)\tPrec 100.000% (99.363%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.280 (0.280)\tLoss 0.1074 (0.1074)\tPrec 97.656% (97.656%)\n",
      " * Prec 90.620% \n",
      "best acc: 90.770000\n",
      "Epoch: [109][0/391]\tTime 0.686 (0.686)\tData 0.638 (0.638)\tLoss 0.0044 (0.0044)\tPrec 100.000% (100.000%)\n",
      "Epoch: [109][200/391]\tTime 0.035 (0.043)\tData 0.002 (0.006)\tLoss 0.0495 (0.0168)\tPrec 99.219% (99.502%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.387 (0.387)\tLoss 0.0773 (0.0773)\tPrec 96.875% (96.875%)\n",
      " * Prec 90.830% \n",
      "best acc: 90.830000\n",
      "Epoch: [110][0/391]\tTime 0.544 (0.544)\tData 0.497 (0.497)\tLoss 0.0204 (0.0204)\tPrec 98.438% (98.438%)\n",
      "Epoch: [110][200/391]\tTime 0.040 (0.043)\tData 0.002 (0.005)\tLoss 0.0048 (0.0188)\tPrec 100.000% (99.351%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.519 (0.519)\tLoss 0.1087 (0.1087)\tPrec 96.875% (96.875%)\n",
      " * Prec 90.620% \n",
      "best acc: 90.830000\n",
      "Epoch: [111][0/391]\tTime 0.591 (0.591)\tData 0.552 (0.552)\tLoss 0.0113 (0.0113)\tPrec 100.000% (100.000%)\n",
      "Epoch: [111][200/391]\tTime 0.042 (0.042)\tData 0.002 (0.005)\tLoss 0.0350 (0.0178)\tPrec 98.438% (99.394%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.347 (0.347)\tLoss 0.1158 (0.1158)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.750% \n",
      "best acc: 90.830000\n",
      "Epoch: [112][0/391]\tTime 0.452 (0.452)\tData 0.403 (0.403)\tLoss 0.0023 (0.0023)\tPrec 100.000% (100.000%)\n",
      "Epoch: [112][200/391]\tTime 0.040 (0.042)\tData 0.002 (0.004)\tLoss 0.0076 (0.0172)\tPrec 100.000% (99.429%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.497 (0.497)\tLoss 0.1081 (0.1081)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.830% \n",
      "best acc: 90.830000\n",
      "Epoch: [113][0/391]\tTime 0.614 (0.614)\tData 0.565 (0.565)\tLoss 0.0070 (0.0070)\tPrec 100.000% (100.000%)\n",
      "Epoch: [113][200/391]\tTime 0.041 (0.043)\tData 0.002 (0.005)\tLoss 0.0067 (0.0190)\tPrec 100.000% (99.394%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.323 (0.323)\tLoss 0.1318 (0.1318)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.690% \n",
      "best acc: 90.830000\n",
      "Epoch: [114][0/391]\tTime 0.604 (0.604)\tData 0.556 (0.556)\tLoss 0.0207 (0.0207)\tPrec 99.219% (99.219%)\n",
      "Epoch: [114][200/391]\tTime 0.042 (0.042)\tData 0.002 (0.005)\tLoss 0.0292 (0.0169)\tPrec 99.219% (99.444%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.403 (0.403)\tLoss 0.1109 (0.1109)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.700% \n",
      "best acc: 90.830000\n",
      "Epoch: [115][0/391]\tTime 0.623 (0.623)\tData 0.568 (0.568)\tLoss 0.0097 (0.0097)\tPrec 99.219% (99.219%)\n",
      "Epoch: [115][200/391]\tTime 0.041 (0.043)\tData 0.002 (0.005)\tLoss 0.0027 (0.0154)\tPrec 100.000% (99.479%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.211 (0.211)\tLoss 0.1257 (0.1257)\tPrec 93.750% (93.750%)\n",
      " * Prec 90.660% \n",
      "best acc: 90.830000\n",
      "Epoch: [116][0/391]\tTime 0.498 (0.498)\tData 0.447 (0.447)\tLoss 0.0361 (0.0361)\tPrec 99.219% (99.219%)\n",
      "Epoch: [116][200/391]\tTime 0.036 (0.043)\tData 0.002 (0.005)\tLoss 0.0019 (0.0181)\tPrec 100.000% (99.440%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.528 (0.528)\tLoss 0.1216 (0.1216)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.750% \n",
      "best acc: 90.830000\n",
      "Epoch: [117][0/391]\tTime 0.575 (0.575)\tData 0.527 (0.527)\tLoss 0.0090 (0.0090)\tPrec 99.219% (99.219%)\n",
      "Epoch: [117][200/391]\tTime 0.040 (0.043)\tData 0.002 (0.005)\tLoss 0.0375 (0.0177)\tPrec 99.219% (99.386%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.247 (0.247)\tLoss 0.1061 (0.1061)\tPrec 97.656% (97.656%)\n",
      " * Prec 90.760% \n",
      "best acc: 90.830000\n",
      "Epoch: [118][0/391]\tTime 0.502 (0.502)\tData 0.453 (0.453)\tLoss 0.0199 (0.0199)\tPrec 99.219% (99.219%)\n",
      "Epoch: [118][200/391]\tTime 0.041 (0.043)\tData 0.002 (0.005)\tLoss 0.0112 (0.0172)\tPrec 99.219% (99.444%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.197 (0.197)\tLoss 0.1463 (0.1463)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.680% \n",
      "best acc: 90.830000\n",
      "Epoch: [119][0/391]\tTime 0.600 (0.600)\tData 0.550 (0.550)\tLoss 0.0256 (0.0256)\tPrec 98.438% (98.438%)\n",
      "Epoch: [119][200/391]\tTime 0.040 (0.043)\tData 0.002 (0.005)\tLoss 0.0139 (0.0168)\tPrec 99.219% (99.483%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.397 (0.397)\tLoss 0.1231 (0.1231)\tPrec 94.531% (94.531%)\n",
      " * Prec 90.630% \n",
      "best acc: 90.830000\n",
      "Epoch: [120][0/391]\tTime 0.463 (0.463)\tData 0.415 (0.415)\tLoss 0.0051 (0.0051)\tPrec 100.000% (100.000%)\n",
      "Epoch: [120][200/391]\tTime 0.040 (0.042)\tData 0.002 (0.004)\tLoss 0.0381 (0.0162)\tPrec 99.219% (99.425%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.429 (0.429)\tLoss 0.1418 (0.1418)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.630% \n",
      "best acc: 90.830000\n",
      "Epoch: [121][0/391]\tTime 0.702 (0.702)\tData 0.654 (0.654)\tLoss 0.0094 (0.0094)\tPrec 100.000% (100.000%)\n",
      "Epoch: [121][200/391]\tTime 0.041 (0.044)\tData 0.003 (0.006)\tLoss 0.0112 (0.0178)\tPrec 100.000% (99.405%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.401 (0.401)\tLoss 0.1108 (0.1108)\tPrec 96.875% (96.875%)\n",
      " * Prec 90.670% \n",
      "best acc: 90.830000\n",
      "Epoch: [122][0/391]\tTime 0.505 (0.505)\tData 0.456 (0.456)\tLoss 0.0114 (0.0114)\tPrec 99.219% (99.219%)\n",
      "Epoch: [122][200/391]\tTime 0.040 (0.043)\tData 0.002 (0.005)\tLoss 0.0067 (0.0153)\tPrec 100.000% (99.499%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.536 (0.536)\tLoss 0.1183 (0.1183)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.540% \n",
      "best acc: 90.830000\n",
      "Epoch: [123][0/391]\tTime 0.732 (0.732)\tData 0.698 (0.698)\tLoss 0.0119 (0.0119)\tPrec 100.000% (100.000%)\n",
      "Epoch: [123][200/391]\tTime 0.040 (0.044)\tData 0.002 (0.006)\tLoss 0.0468 (0.0173)\tPrec 97.656% (99.448%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.392 (0.392)\tLoss 0.1493 (0.1493)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.640% \n",
      "best acc: 90.830000\n",
      "Epoch: [124][0/391]\tTime 0.539 (0.539)\tData 0.491 (0.491)\tLoss 0.0101 (0.0101)\tPrec 99.219% (99.219%)\n",
      "Epoch: [124][200/391]\tTime 0.042 (0.043)\tData 0.003 (0.005)\tLoss 0.0040 (0.0151)\tPrec 100.000% (99.471%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.415 (0.415)\tLoss 0.1294 (0.1294)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.740% \n",
      "best acc: 90.830000\n",
      "Epoch: [125][0/391]\tTime 0.673 (0.673)\tData 0.627 (0.627)\tLoss 0.0016 (0.0016)\tPrec 100.000% (100.000%)\n",
      "Epoch: [125][200/391]\tTime 0.040 (0.044)\tData 0.002 (0.005)\tLoss 0.0083 (0.0176)\tPrec 100.000% (99.464%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.512 (0.512)\tLoss 0.1254 (0.1254)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.670% \n",
      "best acc: 90.830000\n",
      "Epoch: [126][0/391]\tTime 0.556 (0.556)\tData 0.509 (0.509)\tLoss 0.0968 (0.0968)\tPrec 96.875% (96.875%)\n",
      "Epoch: [126][200/391]\tTime 0.039 (0.043)\tData 0.003 (0.005)\tLoss 0.0073 (0.0155)\tPrec 100.000% (99.429%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.236 (0.236)\tLoss 0.1066 (0.1066)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.650% \n",
      "best acc: 90.830000\n",
      "Epoch: [127][0/391]\tTime 0.596 (0.596)\tData 0.548 (0.548)\tLoss 0.0154 (0.0154)\tPrec 99.219% (99.219%)\n",
      "Epoch: [127][200/391]\tTime 0.041 (0.042)\tData 0.002 (0.005)\tLoss 0.0151 (0.0168)\tPrec 99.219% (99.405%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.277 (0.277)\tLoss 0.1161 (0.1161)\tPrec 96.875% (96.875%)\n",
      " * Prec 90.650% \n",
      "best acc: 90.830000\n",
      "Epoch: [128][0/391]\tTime 0.452 (0.452)\tData 0.403 (0.403)\tLoss 0.0095 (0.0095)\tPrec 100.000% (100.000%)\n",
      "Epoch: [128][200/391]\tTime 0.040 (0.042)\tData 0.002 (0.004)\tLoss 0.0094 (0.0172)\tPrec 100.000% (99.413%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.268 (0.268)\tLoss 0.1220 (0.1220)\tPrec 96.875% (96.875%)\n",
      " * Prec 90.580% \n",
      "best acc: 90.830000\n",
      "Epoch: [129][0/391]\tTime 0.490 (0.490)\tData 0.442 (0.442)\tLoss 0.0207 (0.0207)\tPrec 99.219% (99.219%)\n",
      "Epoch: [129][200/391]\tTime 0.040 (0.043)\tData 0.002 (0.005)\tLoss 0.0110 (0.0146)\tPrec 100.000% (99.495%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.299 (0.299)\tLoss 0.1138 (0.1138)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.680% \n",
      "best acc: 90.830000\n"
     ]
    }
   ],
   "source": [
    "# This cell won't be given, but students will complete the training\n",
    "\n",
    "'''lr = 4e-2\n",
    "weight_decay = 1e-4\n",
    "epochs = 130\n",
    "best_prec = 0\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "fdir = 'result/'+str(model_name)\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "        \n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 9083/10000 (91%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = \"result/VGG16_quant/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "spoken-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_q = model.features[27].weight_q\n",
    "w_alpha = model.features[27].weight_quant.wgt_alpha\n",
    "w_bit = 4\n",
    "\n",
    "weight_int = weight_q / (w_alpha / (2**(w_bit-1)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6788b8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 -th layer prehooked\n",
      "7 -th layer prehooked\n",
      "12 -th layer prehooked\n",
      "16 -th layer prehooked\n",
      "21 -th layer prehooked\n",
      "25 -th layer prehooked\n",
      "29 -th layer prehooked\n",
      "34 -th layer prehooked\n",
      "38 -th layer prehooked\n",
      "41 -th layer prehooked\n",
      "46 -th layer prehooked\n",
      "50 -th layer prehooked\n",
      "54 -th layer prehooked\n"
     ]
    }
   ],
   "source": [
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "        \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "i = 0\n",
    "\n",
    "for layer in model.modules():\n",
    "    i = i+1\n",
    "    if isinstance(layer, QuantConv2d):\n",
    "        print(i,\"-th layer prehooked\")\n",
    "        layer.register_forward_pre_hook(save_output)             \n",
    "####################################################\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "out = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "interior-oxygen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  1.0000,  5.0000,  7.0000],\n",
      "          [ 2.0000,  1.0000,  0.0000,  0.0000],\n",
      "          [ 4.0000,  6.0000,  7.0000,  8.0000],\n",
      "          [ 5.0000,  6.0000,  6.0000,  6.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.0000,  9.0000,  9.0000,  7.0000],\n",
      "          [ 5.0000,  6.0000,  6.0000,  6.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  4.0000]],\n",
      "\n",
      "         [[ 4.0000,  5.0000,  4.0000,  1.0000],\n",
      "          [ 4.0000,  4.0000,  1.0000,  0.0000],\n",
      "          [ 3.0000,  4.0000,  2.0000,  1.0000],\n",
      "          [ 0.0000,  1.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 1.0000,  2.0000,  2.0000,  3.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 4.0000,  4.0000,  0.0000,  0.0000],\n",
      "          [ 9.0000, 15.0000, 10.0000,  4.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  1.0000,  5.0000,  8.0000],\n",
      "          [ 0.0000,  0.0000,  3.0000,  9.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  1.0000,  3.0000,  4.0000],\n",
      "          [ 0.0000,  0.0000,  2.0000,  5.0000],\n",
      "          [ 0.0000,  0.0000,  1.0000,  4.0000],\n",
      "          [ 0.0000,  0.0000,  1.0000,  3.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 2.0000,  3.0000,  2.0000,  1.0000],\n",
      "          [ 2.0000,  2.0000,  3.0000,  0.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 6.0000,  9.0000,  6.0000,  4.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0000,  0.0000,  2.0000,  6.0000],\n",
      "          [ 2.0000,  2.0000,  2.0000,  7.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "         [[ 0.0000,  1.0000,  3.0000,  3.0000],\n",
      "          [ 0.0000,  0.0000,  2.0000,  4.0000],\n",
      "          [ 0.0000,  0.0000,  2.0000,  4.0000],\n",
      "          [ 2.0000,  1.0000,  2.0000,  2.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 1.0000,  3.0000,  5.0000,  8.0000],\n",
      "          [ 3.0000,  2.0000,  0.0000,  3.0000],\n",
      "          [ 5.0000,  2.0000,  0.0000,  3.0000],\n",
      "          [ 5.0000,  2.0000,  0.0000,  1.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0000,  0.0000,  0.0000,  3.0000],\n",
      "          [ 2.0000,  0.0000,  0.0000,  5.0000],\n",
      "          [ 4.0000,  1.0000,  0.0000,  3.0000],\n",
      "          [ 7.0000,  5.0000,  1.0000,  4.0000]],\n",
      "\n",
      "         [[ 1.0000,  2.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  3.0000,  1.0000],\n",
      "          [ 3.0000,  4.0000,  6.0000,  2.0000],\n",
      "          [ 2.0000,  4.0000,  5.0000,  2.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 3.0000,  4.0000,  1.0000,  0.0000],\n",
      "          [ 2.0000,  3.0000,  0.0000,  0.0000],\n",
      "          [ 6.0000,  8.0000,  6.0000,  3.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  2.0000,  8.0000],\n",
      "          [ 0.0000,  0.0000,  3.0000,  9.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  4.0000],\n",
      "          [ 2.0000,  2.0000,  2.0000,  5.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  1.0000,  4.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  3.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  1.0000,  4.0000,  5.0000],\n",
      "          [ 1.0000,  4.0000,  5.0000,  3.0000],\n",
      "          [ 1.0000,  2.0000,  2.0000,  1.0000],\n",
      "          [ 6.0000,  7.0000,  6.0000,  5.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.0000,  8.0000, 10.0000,  9.0000],\n",
      "          [ 3.0000,  6.0000,  7.0000,  9.0000],\n",
      "          [ 1.0000,  3.0000,  1.0000,  5.0000],\n",
      "          [ 4.0000,  4.0000,  2.0000,  5.0000]],\n",
      "\n",
      "         [[ 3.0000,  2.0000,  2.0000,  2.0000],\n",
      "          [ 3.0000,  1.0000,  0.0000,  0.0000],\n",
      "          [ 3.0000,  1.0000,  2.0000,  1.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "act = save_output.outputs[8][0]\n",
    "act_alpha  = model.features[27].act_alpha\n",
    "act_bit = 4\n",
    "act_quant_fn = act_quantization(act_bit)\n",
    "\n",
    "act_q = act_quant_fn(act, act_alpha)\n",
    "\n",
    "act_int = act_q / (act_alpha / (2**act_bit-1))\n",
    "print(act_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ranging-porter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0000,  2.2632,  5.0029,  4.7051],\n",
      "          [ 0.0000,  0.6551,  4.4669,  6.1345],\n",
      "          [ 4.2286, 12.0308, 15.9020, 10.8396],\n",
      "          [ 4.2882,  6.0154,  6.5514,  0.3573]],\n",
      "\n",
      "         [[ 5.6580,  5.8963,  7.1470,  1.6676],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 4.7647,  0.5360,  0.0000,  2.8588],\n",
      "          [ 2.6801,  0.0000,  0.0000,  2.7992],\n",
      "          [ 0.9529,  1.6081,  3.1566, 10.7205],\n",
      "          [ 3.9308,  8.9933,  9.8867, 14.7704]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  2.9184,  6.1345,  4.2286],\n",
      "          [ 1.1912,  6.9087, 13.6984, 13.2219],\n",
      "          [ 6.3727, 14.4726, 15.2469,  7.3257],\n",
      "          [ 5.1816, 11.9116, 13.8771,  8.3977]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[10.9587, 15.2469, 15.5447,  9.7080],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.2978,  0.0000,  1.5485,  2.5014]]],\n",
      "\n",
      "\n",
      "        [[[10.3036,  8.2190,  4.1095,  1.5485],\n",
      "          [ 8.1595,  3.6926,  0.0000,  0.0000],\n",
      "          [ 0.0000,  2.5610,  0.2382,  1.4294],\n",
      "          [ 5.4198,  9.2911,  7.9808,  5.7771]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 4.8838,  0.1191,  0.0000,  0.0000],\n",
      "          [ 5.2411,  3.6926,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  4.0500,  3.1566],\n",
      "          [ 0.0000,  6.0154,  8.5764,  7.5043],\n",
      "          [12.3881, 21.0240, 15.6042,  4.5860],\n",
      "          [10.4822, 17.5697, 14.6513,  5.0624]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[16.8550, 21.7983, 16.6167,  6.4323],\n",
      "          [10.7205, 18.1057, 14.9491,  3.3353],\n",
      "          [ 0.0000,  2.9779,  5.3007,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  9.8867,  8.2786],\n",
      "          [ 0.0000,  3.8713,  8.8146,  0.0000],\n",
      "          [ 1.4890,  5.5389,  4.4073,  0.0000],\n",
      "          [ 2.8588,  0.6551,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 7.0874,  7.5043,  6.3132,  2.5610],\n",
      "          [ 6.5514,  2.4419,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  1.0720],\n",
      "          [ 4.3477,  6.9087,  5.3602,  4.1095]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 4.2882,  0.0000,  0.0000,  0.0000],\n",
      "          [ 2.3228,  1.4890,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  2.5610,  2.2632],\n",
      "          [ 0.0000,  6.7896,  8.8742,  6.9683],\n",
      "          [ 6.3132, 11.2565,  9.6484,  3.8713],\n",
      "          [ 6.1345,  8.9933,  8.3977,  3.5139]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[10.3036, 15.4256, 13.1028,  5.5389],\n",
      "          [ 6.4918, 12.0308, 10.4822,  1.3103],\n",
      "          [ 0.0000,  0.4765,  3.5735,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.1191,  6.6705,  4.1095],\n",
      "          [ 0.8934,  3.0375,  7.4448,  0.0000],\n",
      "          [ 0.8934,  1.8463,  3.6926,  0.0000],\n",
      "          [ 1.3103,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 4.1095,  7.0279,  6.6110,  6.7301],\n",
      "          [ 4.2286,  5.1220,  1.3103,  0.4765],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.1191],\n",
      "          [ 0.7147,  2.4419,  4.4073,  1.6081]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  2.2632,  2.1441],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 3.9308,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.1191,  4.7051,  7.8021],\n",
      "          [ 5.1220,  2.4419,  1.4890,  7.6234],\n",
      "          [ 8.9933,  5.8963,  0.2978,  4.2882],\n",
      "          [ 4.4073,  2.2632,  0.0000,  0.3573]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.0749, 10.4822,  7.7426,  1.2507],\n",
      "          [ 2.3823, 10.8991, 10.5418,  3.7522],\n",
      "          [ 0.0000,  4.4669,  3.4544,  0.0000],\n",
      "          [ 2.0845,  9.5293,  8.5168,  3.6926]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  2.3823],\n",
      "          [ 1.7272,  0.5360,  0.8934,  2.0250],\n",
      "          [ 8.6359,  5.1220,  3.9308,  1.1912],\n",
      "          [ 4.9433,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 6.4323,  8.9337,  3.8713,  0.0000],\n",
      "          [ 9.3506,  8.3977,  0.0000,  0.0000],\n",
      "          [ 3.8713,  2.3228,  0.0000,  0.0000],\n",
      "          [ 2.9779,  3.3353,  2.7992,  1.2507]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.4169,  0.0000,  0.0000,  0.0000],\n",
      "          [ 2.3823,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  2.2037],\n",
      "          [ 0.0000,  5.8963,  9.6484,  8.4573],\n",
      "          [ 4.8838, 12.0308, 10.9587,  5.0624],\n",
      "          [ 7.3257, 13.9366, 11.1969,  5.8367]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[12.2690, 18.1057, 16.7358,  6.0749],\n",
      "          [10.4822, 14.4726, 13.7579,  0.5956],\n",
      "          [ 3.2161,  7.9212,  9.4698,  0.0000],\n",
      "          [ 0.0000,  0.1787,  2.9779,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  9.3506,  8.6359],\n",
      "          [ 0.0000,  0.2382,  9.9462,  2.4419],\n",
      "          [ 2.4419,  6.1941,  9.9462,  0.4765],\n",
      "          [ 3.6330,  4.2286,  4.3477,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4890,  3.4544,  5.6580,  6.0154],\n",
      "          [ 0.0000,  0.1787,  0.8934,  0.3573],\n",
      "          [ 2.7397,  5.7176,  7.3257,  4.5860],\n",
      "          [ 5.3007,  6.1345,  6.9683,  1.4890]],\n",
      "\n",
      "         [[ 3.6330,  7.4448, 11.1374,  3.5139],\n",
      "          [ 0.0000,  0.0000,  2.2632,  0.0000],\n",
      "          [ 0.0596,  0.0000,  0.5360,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 1.4890,  2.9184,  3.9308,  3.2161],\n",
      "          [ 0.0000,  2.4419,  2.0845,  6.3132],\n",
      "          [ 0.0000,  0.0000,  0.0000,  4.3477],\n",
      "          [ 4.7647,  9.6484,  8.2190,  9.7675]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.8117,  7.8617, 10.8396,  7.5043],\n",
      "          [ 0.5360,  4.5860,  8.5764,  4.2286],\n",
      "          [ 3.6330, 12.4477, 16.9741,  7.8021],\n",
      "          [ 2.2632,  7.2065,  9.2315,  3.6330]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 7.1470, 15.4851, 20.8454, 13.2219],\n",
      "          [ 0.0596,  2.9779,  6.3727,  0.0000],\n",
      "          [ 2.0250,  2.9779,  3.3353,  0.0000],\n",
      "          [ 3.0970,  1.5485,  2.7992,  0.0000]]]], device='cuda:0',\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "conv_int = torch.nn.Conv2d(in_channels = 8, out_channels=8, kernel_size = 3, padding=1)\n",
    "conv_int.weight = torch.nn.parameter.Parameter(weight_int)\n",
    "conv_int.bias = model.features[27].bias\n",
    "output_int = conv_int(act_int)\n",
    "a = nn.ReLU()\n",
    "output_recovered = a(output_int * (act_alpha / (2**act_bit-1)) * (w_alpha / (2**(w_bit-1)-1)))\n",
    "print(output_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "157dffd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6209e-07, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "difference = abs( save_output.outputs[9][0] - output_recovered )\n",
    "print(difference.mean())  ## It should be small, e.g.,2.3 in my trainned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "entitled-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "# act_int.size = torch.Size([128, 64, 32, 32])  <- batch_size, input_ch, ni, nj\n",
    "a_int = act_int[0,:,:,:]  # pick only one input out of batch\n",
    "# a_int.size() = [64, 32, 32]\n",
    "\n",
    "# conv_int.weight.size() = torch.Size([64, 64, 3, 3])  <- output_ch, input_ch, ki, kj\n",
    "w_int = torch.reshape(weight_int, (weight_int.size(0), weight_int.size(1), -1))  # merge ki, kj index to kij\n",
    "# w_int.weight.size() = torch.Size([64, 64, 9])\n",
    "                      \n",
    "padding = 1\n",
    "stride = 1\n",
    "array_size = 8 # row and column number\n",
    "\n",
    "nig = range(a_int.size(1))  ## ni group\n",
    "njg = range(a_int.size(2))  ## nj group\n",
    "\n",
    "icg = range(int(w_int.size(1)))  ## input channel \n",
    "ocg = range(int(w_int.size(0)))  ## output channel\n",
    "\n",
    "ic_tileg = range(int(len(icg)/array_size))\n",
    "oc_tileg = range(int(len(ocg)/array_size))\n",
    "\n",
    "kijg = range(w_int.size(2))\n",
    "ki_dim = int(math.sqrt(w_int.size(2)))  ## Kernel's 1 dim size\n",
    "\n",
    "######## Padding before Convolution #######\n",
    "a_pad = torch.zeros(len(icg), len(nig)+padding*2, len(nig)+padding*2).cuda()\n",
    "# a_pad.size() = [64, 32+2pad, 32+2pad]\n",
    "a_pad[ :, padding:padding+len(nig), padding:padding+len(njg)] = a_int.cuda()\n",
    "a_pad = torch.reshape(a_pad, (a_pad.size(0), -1))\n",
    "# a_pad.size() = [64, (32+2pad)*(32+2pad)]\n",
    "\n",
    "\n",
    "a_tile = torch.zeros(len(ic_tileg), array_size,    a_pad.size(1)).cuda() \n",
    "w_tile = torch.zeros(len(oc_tileg)*len(ic_tileg), array_size, array_size, len(kijg)).cuda() \n",
    "\n",
    "for ic_tile in ic_tileg:\n",
    "    a_tile[ic_tile,:,:] = a_pad[ic_tile*array_size:(ic_tile+1)*array_size,:]\n",
    "\n",
    "for ic_tile in ic_tileg:\n",
    "    for oc_tile in oc_tileg:\n",
    "        w_tile[oc_tile*len(oc_tileg) + ic_tile,:,:,:] = w_int[oc_tile*array_size:(oc_tile+1)*array_size, ic_tile*array_size:(ic_tile+1)*array_size, :]\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "p_nijg = range(a_pad.size(1)) ## psum nij group\n",
    "\n",
    "psum = torch.zeros(len(ic_tileg), len(oc_tileg), array_size, len(p_nijg), len(kijg)).cuda() \n",
    "\n",
    "for kij in kijg:\n",
    "    for ic_tile in ic_tileg:       # Tiling into array_sizeXarray_size array\n",
    "        for oc_tile in oc_tileg:   # Tiling into array_sizeXarray_size array        \n",
    "            for nij in p_nijg:       # time domain, sequentially given input\n",
    "                    m = nn.Linear(array_size, array_size, bias=False)\n",
    "                    #m.weight = torch.nn.Parameter(w_int[oc_tile*array_size:(oc_tile+1)*array_size, ic_tile*array_size:(ic_tile+1)*array_size, kij])\n",
    "                    m.weight = torch.nn.Parameter(w_tile[len(oc_tileg)*oc_tile+ic_tile,:,:,kij])\n",
    "                    psum[ic_tile, oc_tile, :, nij, kij] = m(a_tile[ic_tile,:,nij]).cuda()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "minimal-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "a_pad_ni_dim = int(math.sqrt(a_pad.size(1))) # 32\n",
    "\n",
    "o_ni_dim = int((a_pad_ni_dim - (ki_dim- 1) - 1)/stride + 1)\n",
    "o_nijg = range(o_ni_dim**2)    \n",
    "    \n",
    "out = torch.zeros(len(ocg), len(o_nijg)).cuda()\n",
    "  \n",
    "   \n",
    "### SFP accumulation ###\n",
    "for o_nij in o_nijg: \n",
    "    for kij in kijg:\n",
    "        for ic_tile in ic_tileg:    \n",
    "            for oc_tile in oc_tileg:   \n",
    "                out[oc_tile*array_size:(oc_tile+1)*array_size, o_nij] = out[oc_tile*array_size:(oc_tile+1)*array_size, o_nij] + \\\n",
    "                psum[ic_tile, oc_tile, :, int(o_nij/o_ni_dim)*a_pad_ni_dim + o_nij%o_ni_dim + int(kij/ki_dim)*a_pad_ni_dim + kij%ki_dim, kij]\n",
    "                ## 4th index = (int(o_nij/30)*32 + o_nij%30) + (int(kij/3)*32 + kij%3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90a4dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "address = torch.zeros(len(ic_tileg),len(oc_tileg),len(o_nijg),len(kijg)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db7c93c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for o_nij in o_nijg: \n",
    "    for kij in kijg:\n",
    "        for ic_tile in ic_tileg:    \n",
    "            for oc_tile in oc_tileg:   \n",
    "                address[ic_tile, oc_tile,o_nij,kij] = int(o_nij/o_ni_dim)*a_pad_ni_dim + o_nij%o_ni_dim + int(kij/ki_dim)*a_pad_ni_dim + kij%ki_dim\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf132f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.7180e-05, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_2D = torch.reshape(out, (out.size(0), o_ni_dim, -1))\n",
    "difference = (out_2D - output_int[0,:,:,:])\n",
    "print(difference.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91fce335",
   "metadata": {},
   "outputs": [],
   "source": [
    "### show this cell partially. The following cells should be printed by students ###\n",
    "tile_id = 0 \n",
    "nij = 200 # just a random number\n",
    "X = a_tile[tile_id,:,:]  # [tile_num, array row num, time_steps]\n",
    "\n",
    "bit_precision = 4\n",
    "file = open('activation.txt', 'w') #write to file\n",
    "file.write('#time0row7[msb-lsb],time0row6[msb-lst],....,time0row0[msb-lst]#\\n')\n",
    "file.write('#time1row7[msb-lsb],time1row6[msb-lst],....,time1row0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(X.size(1)):  # time step\n",
    "    for j in range(X.size(0)): # row #\n",
    "        X_bin = '{0:04b}'.format(round(X[7-j,i].item()))\n",
    "        for k in range(bit_precision):\n",
    "            file.write(X_bin[k])        \n",
    "        #file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close() #close file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ddb8a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Complete this cell ###\n",
    "tile_id = 0 \n",
    "kij = 0\n",
    "W = w_tile[tile_id,:,:,:]  # w_tile[tile_num, array col num, array row num, kij]\n",
    "\n",
    "\n",
    "bit_precision = 4\n",
    "\n",
    "\n",
    "for x in range(W.size(2)):\n",
    "    filename = f'weight{x}.txt'\n",
    "    \n",
    "    # Open the file for writing\n",
    "    with open(filename, 'w') as file:\n",
    "        file = open(filename, 'w') #write to file\n",
    "    file.write('#col0row7[msb-lsb],col0row6[msb-lst],....,col0row0[msb-lst]#\\n')\n",
    "    file.write('#col1row7[msb-lsb],col1row6[msb-lst],....,col1row0[msb-lst]#\\n')\n",
    "    file.write('#................#\\n')\n",
    "    for i in range(W.size(1)):  # time step\n",
    "        for j in range(W.size(0)): # row #\n",
    "            W_bin = '{0:04b}'.format(round(W[7-j,i,x].item()))\n",
    "            W_list = list(W_bin)\n",
    "            if(W_list[0] == '-'):\n",
    "                W_list[0] = '1'\n",
    "\n",
    "\n",
    "                if(W_list[1] == '1'):\n",
    "                    W_list[1] = '0'\n",
    "                else:\n",
    "                    W_list[1] = '1'\n",
    "\n",
    "                if(W_list[2] == '1'):\n",
    "                    W_list[2] = '0'\n",
    "                else:\n",
    "                    W_list[2] = '1'\n",
    "\n",
    "                if(W_list[3] == '1'):\n",
    "                    W_list[3] = '0'\n",
    "                else:\n",
    "                    W_list[3] = '1'\n",
    "                modified_W_bin = ''.join(W_list)\n",
    "                W_bin = (bin(int( modified_W_bin, 2) + 1)[2:])\n",
    "            for k in range(bit_precision):\n",
    "                file.write(W_bin[k])        \n",
    "            #file.write(' ')  # for visibility with blank between words, you can use\n",
    "        file.write('\\n')\n",
    "file.close() #close file   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc5d2deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Complete this cell ###\n",
    "ic_tile_id = 0 \n",
    "oc_tile_id = 0 \n",
    "\n",
    "\n",
    "kij = 0\n",
    "nij = 200\n",
    "psum_tile = psum[ic_tile_id,oc_tile_id,:,:]  \n",
    "# psum[len(ic_tileg), len(oc_tileg), array_size, len(p_nijg), len(kijg)]\n",
    "\n",
    "\n",
    "bit_precision = 16\n",
    "\n",
    "for x in range(psum_tile.size(2)):\n",
    "    filename = f'psum{x}.txt'\n",
    "    # Open the file for writing\n",
    "    with open(filename, 'w') as file:\n",
    "        file = open(filename, 'w') #write to file\n",
    "    file.write('#time0col7[msb-lsb],time0col6[msb-lst],....,time0col0[msb-lst]#\\n')\n",
    "    file.write('#time1col7[msb-lsb],time1col6[msb-lst],....,time1col0[msb-lst]#\\n')\n",
    "    file.write('#................#\\n')\n",
    "\n",
    "    for i in range(psum_tile.size(1)):  # time step\n",
    "        for j in range(psum_tile.size(0)): # row #\n",
    "            P_bin = '{0:016b}'.format(round(psum_tile[7-j,i,x].item()))\n",
    "            P_list = list(P_bin)\n",
    "            if(P_list[0] == '-'):\n",
    "                P_list[0] = '1'\n",
    "\n",
    "\n",
    "                if(P_list[1] == '1'):\n",
    "                    P_list[1] = '0'\n",
    "                else:\n",
    "                    P_list[1] = '1'\n",
    "\n",
    "                if(P_list[2] == '1'):\n",
    "                    P_list[2] = '0'\n",
    "                else:\n",
    "                    P_list[2] = '1'\n",
    "\n",
    "                if(P_list[3] == '1'):\n",
    "                    P_list[3] = '0'\n",
    "                else:\n",
    "                    P_list[3] = '1'\n",
    "\n",
    "                if(P_list[4] == '1'):\n",
    "                    P_list[4] = '0'\n",
    "                else:\n",
    "                    P_list[4] = '1'\n",
    "\n",
    "                if(P_list[5] == '1'):\n",
    "                    P_list[5] = '0'\n",
    "                else:\n",
    "                    P_list[5] = '1'\n",
    "\n",
    "                if(P_list[6] == '1'):\n",
    "                    P_list[6] = '0'\n",
    "                else:\n",
    "                    P_list[6] = '1'\n",
    "\n",
    "                if(P_list[7] == '1'):\n",
    "                    P_list[7] = '0'\n",
    "                else:\n",
    "                    P_list[7] = '1'\n",
    "\n",
    "                if(P_list[8] == '1'):\n",
    "                    P_list[8] = '0'\n",
    "                else:\n",
    "                    P_list[8] = '1'\n",
    "\n",
    "                if(P_list[9] == '1'):\n",
    "                    P_list[9] = '0'\n",
    "                else:\n",
    "                    P_list[9] = '1'\n",
    "\n",
    "                if(P_list[10] == '1'):\n",
    "                    P_list[10] = '0'\n",
    "                else:\n",
    "                    P_list[10] = '1'\n",
    "\n",
    "                if(P_list[11] == '1'):\n",
    "                    P_list[11] = '0'\n",
    "                else:\n",
    "                    P_list[11] = '1'\n",
    "\n",
    "                if(P_list[12] == '1'):\n",
    "                    P_list[12] = '0'\n",
    "                else:\n",
    "                    P_list[12] = '1'\n",
    "                if(P_list[13] == '1'):\n",
    "                    P_list[13] = '0'\n",
    "                else:\n",
    "                    P_list[13] = '1'\n",
    "\n",
    "                if(P_list[14] == '1'):\n",
    "                    P_list[14] = '0'\n",
    "                else:\n",
    "                    P_list[14] = '1'\n",
    "\n",
    "                if(P_list[15] == '1'):\n",
    "                    P_list[15] = '0'\n",
    "                else:\n",
    "                    P_list[15] = '1'\n",
    "\n",
    "                modified_P_bin = ''.join(P_list)\n",
    "                P_bin = (bin(int( modified_P_bin, 2) + 1)[2:])\n",
    "\n",
    "            for k in range(bit_precision):\n",
    "                file.write(P_bin[k])        \n",
    "            #file.write(' ')  # for visibility with blank between words, you can use\n",
    "        file.write('\\n')\n",
    "file.close() #close file   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "961651e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "address = address.reshape([16,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e42d33cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 9])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02e45d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  6.,  7.,  8., 12., 13., 14.],\n",
       "        [ 1.,  2.,  3.,  7.,  8.,  9., 13., 14., 15.],\n",
       "        [ 2.,  3.,  4.,  8.,  9., 10., 14., 15., 16.],\n",
       "        [ 3.,  4.,  5.,  9., 10., 11., 15., 16., 17.],\n",
       "        [ 6.,  7.,  8., 12., 13., 14., 18., 19., 20.],\n",
       "        [ 7.,  8.,  9., 13., 14., 15., 19., 20., 21.],\n",
       "        [ 8.,  9., 10., 14., 15., 16., 20., 21., 22.],\n",
       "        [ 9., 10., 11., 15., 16., 17., 21., 22., 23.],\n",
       "        [12., 13., 14., 18., 19., 20., 24., 25., 26.],\n",
       "        [13., 14., 15., 19., 20., 21., 25., 26., 27.],\n",
       "        [14., 15., 16., 20., 21., 22., 26., 27., 28.],\n",
       "        [15., 16., 17., 21., 22., 23., 27., 28., 29.],\n",
       "        [18., 19., 20., 24., 25., 26., 30., 31., 32.],\n",
       "        [19., 20., 21., 25., 26., 27., 31., 32., 33.],\n",
       "        [20., 21., 22., 26., 27., 28., 32., 33., 34.],\n",
       "        [21., 22., 23., 27., 28., 29., 33., 34., 35.]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17f3be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_precision = 16\n",
    "file = open('address.txt', 'w') #write to file\n",
    "file.write('#time0row7[msb-lsb],time0row6[msb-lst],....,time0row0[msb-lst]#\\n')\n",
    "file.write('#time1row7[msb-lsb],time1row6[msb-lst],....,time1row0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(address.size(0)):  # time step\n",
    "    for j in range(address.size(1)): # row #\n",
    "        X_bin = '{0:016b}'.format(int(address[i,j].item()))\n",
    "        for k in range(bit_precision):\n",
    "            file.write(X_bin[k])        \n",
    "        file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close() #close file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa36d7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
